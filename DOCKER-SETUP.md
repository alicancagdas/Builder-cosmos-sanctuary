# ğŸ³ Docker + Ollama Setup Guide

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Docker ve Docker Compose Kurulumu

```bash
# Docker kurulu olduÄŸundan emin olun
docker --version
docker-compose --version
```

### 2. Proje Dizininde Docker Compose BaÅŸlatma

```bash
# TÃ¼m servisleri baÅŸlat (Ollama + React Native Web)
docker-compose up -d

# LoglarÄ± takip et
docker-compose logs -f
```

### 3. EriÅŸim URL'leri

- **Web App**: http://localhost:3000
- **Ollama API**: http://localhost:11434
- **Development Server**: http://localhost:19006 (dev mode)

## ğŸ”§ GeliÅŸtirme Modunda Ã‡alÄ±ÅŸtÄ±rma

### Development Container

```bash
# Sadece development container'Ä± baÅŸlat
docker-compose up app-dev

# Veya background'da
docker-compose up -d app-dev
```

### Local Development (Docker'sÄ±z)

```bash
# 1. Ollama'yÄ± Docker'da baÅŸlat
docker-compose up -d ollama

# 2. Web uygulamasÄ±nÄ± local'de Ã§alÄ±ÅŸtÄ±r
npm install
npx expo start --web
```

## ğŸ“¦ Ollama Modeli YÃ¼kleme

### Otomatik YÃ¼kleme

Docker Compose baÅŸladÄ±ÄŸÄ±nda Llama3 modeli otomatik yÃ¼klenir. Ä°lk baÅŸlatmada biraz zaman alabilir.

### Manuel Model YÃ¼kleme

```bash
# Container iÃ§inde model yÃ¼kle
docker exec -it codementor-ollama ollama pull llama3

# DiÄŸer modeller
docker exec -it codementor-ollama ollama pull codellama
docker exec -it codementor-ollama ollama pull mistral
```

### Mevcut Modelleri Listele

```bash
docker exec -it codementor-ollama ollama list
```

## ğŸ›  Debug ve Troubleshooting

### Container DurumlarÄ±nÄ± Kontrol Et

```bash
# Ã‡alÄ±ÅŸan container'larÄ± gÃ¶r
docker-compose ps

# LoglarÄ± kontrol et
docker-compose logs ollama
docker-compose logs app
```

### Ollama Servisi KontrolÃ¼

```bash
# Ollama health check
curl http://localhost:11434/api/tags

# Model test
curl http://localhost:11434/api/generate \
  -d '{
    "model": "llama3",
    "prompt": "Hello, how are you?",
    "stream": false
  }'
```

### Container'larÄ± Yeniden BaÅŸlat

```bash
# TÃ¼m servisleri yeniden baÅŸlat
docker-compose restart

# Sadece Ollama'yÄ± yeniden baÅŸlat
docker-compose restart ollama

# Temiz baÅŸlangÄ±Ã§ (volume'larÄ± korur)
docker-compose down && docker-compose up -d
```

### Veri Temizleme

```bash
# Container'larÄ± ve network'Ã¼ sil (volume'larÄ± korur)
docker-compose down

# Volume'larÄ± da sil (model'ler silinir!)
docker-compose down -v

# TÃ¼m container'larÄ± ve image'larÄ± sil
docker-compose down --rmi all -v
```

## ğŸ” Monitoring ve Logs

### Real-time Logs

```bash
# TÃ¼m servislerin loglarÄ±
docker-compose logs -f

# Sadece Ollama loglarÄ±
docker-compose logs -f ollama

# Sadece app loglarÄ±
docker-compose logs -f app
```

### Container Ä°Ã§ine GiriÅŸ

```bash
# Ollama container'Ä±na gir
docker exec -it codementor-ollama bash

# App container'Ä±na gir
docker exec -it codementor-app sh
```

## âš™ï¸ Configuration

### Environment Variables

`docker-compose.yml` dosyasÄ±nda ÅŸu environment variable'larÄ± ayarlayabilirsiniz:

```yaml
environment:
  - OLLAMA_HOST=0.0.0.0
  - OLLAMA_API_URL=http://ollama:11434
  - EXPO_DEVTOOLS_LISTEN_ADDRESS=0.0.0.0
```

### Port DeÄŸiÅŸiklikleri

```yaml
ports:
  - "3001:80" # Web app port deÄŸiÅŸtir
  - "11435:11434" # Ollama port deÄŸiÅŸtir
  - "19007:19006" # Expo web port deÄŸiÅŸtir
```

## ğŸ¯ Production Deployment

### Production Build

```bash
# Production image'Ä±nÄ± build et
docker build -t codementor-ai:latest .

# Run production container
docker run -d \
  -p 3000:80 \
  --link codementor-ollama:ollama \
  codementor-ai:latest
```

### Docker Hub'a Push

```bash
# Tag the image
docker tag codementor-ai:latest username/codementor-ai:latest

# Push to Docker Hub
docker push username/codementor-ai:latest
```

## ğŸ”’ Security Considerations

### Production GÃ¼venlik

- Ollama API'yi external access'ten koru
- Environment variables iÃ§in secret management kullan
- Container'larÄ± non-root user ile Ã§alÄ±ÅŸtÄ±r
- Network isolation uygula

### Firewall KurallarÄ±

```bash
# Sadece localhost'tan Ollama eriÅŸimi
iptables -A INPUT -p tcp --dport 11434 -s 127.0.0.1 -j ACCEPT
iptables -A INPUT -p tcp --dport 11434 -j DROP
```

## ğŸ“Š Performance Tuning

### Ollama Performance

```bash
# GPU support iÃ§in
docker run --gpus all ollama/ollama

# Memory limiti
docker run -m 4g ollama/ollama
```

### Model Optimization

```bash
# Quantized model kullan (daha az RAM)
docker exec -it codementor-ollama ollama pull llama3:7b-q4_0

# Model'i memory'de tut
docker exec -it codementor-ollama ollama run llama3 "warmup"
```

## ğŸ†˜ Common Issues

### Issue: Ollama modeli yÃ¼klenmiyor

```bash
# Solution: Manual model pull
docker exec -it codementor-ollama ollama pull llama3
```

### Issue: React Native web bundle hatasÄ±

```bash
# Solution: Clear cache ve rebuild
docker-compose down
docker-compose build --no-cache app
docker-compose up -d
```

### Issue: Port conflict

```bash
# Solution: Port deÄŸiÅŸtir
docker-compose down
# docker-compose.yml'de port'larÄ± deÄŸiÅŸtir
docker-compose up -d
```

### Issue: Memory yetersiz

```bash
# Solution: Lightweight model kullan
docker exec -it codementor-ollama ollama pull llama3:7b-q4_0
```

Bu konfigÃ¼rasyon ile CodeMentor AI uygulamanÄ±z Llama3 AI desteÄŸi ile Docker'da Ã§alÄ±ÅŸacak! ğŸ‰
