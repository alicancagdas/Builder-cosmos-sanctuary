# ğŸ¤– Ollama AI Setup Guide

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Ollama Servisini BaÅŸlat

```bash
# Development iÃ§in sadece Ollama
docker-compose -f docker-compose.dev.yml up -d

# Service durumunu kontrol et
docker-compose -f docker-compose.dev.yml ps
```

### 2. Llama3 Modelini YÃ¼kle

#### Linux/Mac iÃ§in:

```bash
# Script'i Ã§alÄ±ÅŸtÄ±rÄ±labilir yap
chmod +x scripts/init-ollama.sh

# Modeli yÃ¼kle
./scripts/init-ollama.sh
```

#### Windows iÃ§in:

```bash
# Batch script Ã§alÄ±ÅŸtÄ±r
scripts\init-ollama.bat
```

#### Manuel YÃ¼kleme:

```bash
# Container'a baÄŸlan ve model yÃ¼kle
docker exec -it codementor-ollama-dev ollama pull llama3

# Model listesini kontrol et
docker exec -it codementor-ollama-dev ollama list
```

## ğŸ” Ollama Service KontrolÃ¼

### Health Check

```bash
# Service Ã§alÄ±ÅŸÄ±yor mu?
curl http://localhost:11434/api/tags

# Modellar yÃ¼klÃ¼ mÃ¼?
curl http://localhost:11434/api/tags | jq '.models'
```

### Manual AI Test

```bash
# Basit test
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3",
    "prompt": "C++ pointer nedir? KÄ±saca aÃ§Ä±kla.",
    "stream": false
  }' | jq -r '.response'
```

### Interactive Chat Test

```bash
# Container iÃ§inde interactive chat
docker exec -it codementor-ollama-dev ollama run llama3
```

## ğŸ›  Troubleshooting

### Model YÃ¼klenmiyorsa

```bash
# Container loglarÄ±nÄ± kontrol et
docker logs codementor-ollama-dev

# Container'Ä± yeniden baÅŸlat
docker-compose -f docker-compose.dev.yml restart ollama

# Manual download
docker exec -it codementor-ollama-dev ollama pull llama3
```

### Port Ã‡akÄ±ÅŸmasÄ±

```bash
# Port 11434 kullanÄ±mda mÄ± kontrol et
lsof -i :11434

# Veya Windows'ta
netstat -an | findstr 11434

# FarklÄ± port kullan
docker-compose -f docker-compose.dev.yml down
# docker-compose.dev.yml'de port deÄŸiÅŸtir "11435:11434"
docker-compose -f docker-compose.dev.yml up -d
```

### Memory YetersizliÄŸi

```bash
# Lightweight model kullan
docker exec -it codementor-ollama-dev ollama pull llama3:7b-q4_0

# Model boyutunu kontrol et
docker exec -it codementor-ollama-dev ollama list
```

## ğŸ“Š Performance Monitoring

### Resource Usage

```bash
# Container resource kullanÄ±mÄ±
docker stats codementor-ollama-dev

# Disk usage
docker exec -it codementor-ollama-dev du -sh /root/.ollama
```

### API Response Times

```bash
# Benchmark test
time curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3",
    "prompt": "Hello",
    "stream": false
  }'
```

## ğŸ”§ Alternative Models

### Lightweight Models

```bash
# Daha kÃ¼Ã§Ã¼k model (daha hÄ±zlÄ±)
docker exec -it codementor-ollama-dev ollama pull llama3:7b-q4_0

# Code-specific model
docker exec -it codementor-ollama-dev ollama pull codellama

# Very fast, smaller model
docker exec -it codementor-ollama-dev ollama pull phi3:mini
```

### Model Switching

```bash
# List available models
docker exec -it codementor-ollama-dev ollama list

# Test different model
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "codellama",
    "prompt": "Write a simple C++ hello world program",
    "stream": false
  }'
```

## ğŸ¯ Integration Test

### Full Stack Test

```bash
# 1. Start Ollama
docker-compose -f docker-compose.dev.yml up -d

# 2. Wait and init
sleep 30
./scripts/init-ollama.sh

# 3. Start web app
npm install --legacy-peer-deps
npx expo start --web

# 4. Test AI Chat in browser
# http://localhost:19006
```

## ğŸ“ Development Tips

### Rapid Development

```bash
# Start only what you need
docker-compose -f docker-compose.dev.yml up -d

# Keep Ollama running, restart web app as needed
npx expo start --web --clear
```

### API Development

```bash
# Watch Ollama logs while developing
docker logs -f codementor-ollama-dev

# Test API calls from your code
# Check src/services/ollamaService.ts
```

### Model Caching

```bash
# Keep model loaded for faster responses
docker exec -it codementor-ollama-dev ollama run llama3 "warmup"
```

Bu rehber ile Ollama AI servisiniz sorunsuz Ã§alÄ±ÅŸacak! ğŸ‰
